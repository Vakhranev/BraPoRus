{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaetuyWv0jXhXMguJmbI0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/BraPoRus/blob/main/%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B8_BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NQvk8aS04Yu",
        "outputId": "3f971f74-f2e6-41db-c515-692dc7c76629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 32 файлов в папке files.\n",
            "NVM_BB.cha: средняя длина токена = 4.91, SD = 3.91\n",
            "LDL_BB.cha: средняя длина токена = 5.05, SD = 3.80\n",
            "ENSh_BB.cha: средняя длина токена = 5.15, SD = 3.80\n",
            "NVT-M_BB.cha: средняя длина токена = 5.05, SD = 3.79\n",
            "MAL_BB.cha: средняя длина токена = 5.26, SD = 3.83\n",
            "ANO_BB.cha: средняя длина токена = 5.08, SD = 3.68\n",
            "TYL_BB.cha: средняя длина токена = 5.14, SD = 3.85\n",
            "VDL_BB.cha: средняя длина токена = 5.09, SD = 3.89\n",
            "PPZ_BB.cha: средняя длина токена = 4.73, SD = 3.51\n",
            "AVG_BB.cha: средняя длина токена = 4.73, SD = 3.69\n",
            "MRP_BB.cha: средняя длина токена = 4.72, SD = 3.54\n",
            "AVM_BB.cha: средняя длина токена = 5.16, SD = 3.91\n",
            "GAA_BB.cha: средняя длина токена = 5.20, SD = 3.86\n",
            "SVK_BB.cha: средняя длина токена = 4.77, SD = 3.54\n",
            "EAB_BB.cha: средняя длина токена = 4.72, SD = 3.59\n",
            "TAR_BB.cha: средняя длина токена = 4.86, SD = 3.62\n",
            "PIF_BB.cha: средняя длина токена = 5.38, SD = 3.70\n",
            "OAR_BB.cha: средняя длина токена = 4.93, SD = 3.53\n",
            "EVP_BB.cha: средняя длина токена = 5.16, SD = 3.98\n",
            "AGM_BB.cha: средняя длина токена = 5.14, SD = 3.74\n",
            "VLN_BB.cha: средняя длина токена = 4.95, SD = 3.81\n",
            "IGM_BB.cha: средняя длина токена = 4.89, SD = 3.72\n",
            "EGI_BB.cha: средняя длина токена = 4.93, SD = 3.71\n",
            "TAL_BB.cha: средняя длина токена = 5.08, SD = 3.74\n",
            "MAV_BB.cha: средняя длина токена = 4.60, SD = 3.61\n",
            "ZVH_BB.cha: средняя длина токена = 4.89, SD = 3.49\n",
            "TNK_BB.cha: средняя длина токена = 5.10, SD = 3.88\n",
            "AIL_BB.cha: средняя длина токена = 4.97, SD = 3.77\n",
            "VES_BB.cha: средняя длина токена = 4.66, SD = 3.70\n",
            "HAL_BB.cha: средняя длина токена = 5.04, SD = 3.55\n",
            "EAK_BB.cha: средняя длина токена = 5.36, SD = 3.96\n",
            "NRP_BB.cha: средняя длина токена = 4.77, SD = 3.63\n",
            "\n",
            "Средняя длина токена по всем файлам: 4.93\n",
            "Стандартное отклонение длины токенов по всем файлам: 3.73\n",
            "Найдено 32 файлов в папке files.\n",
            "NVM_BB.cha: средняя длина токена = 4.91, SD = 3.91\n",
            "LDL_BB.cha: средняя длина токена = 5.05, SD = 3.80\n",
            "ENSh_BB.cha: средняя длина токена = 5.15, SD = 3.80\n",
            "NVT-M_BB.cha: средняя длина токена = 5.05, SD = 3.79\n",
            "MAL_BB.cha: средняя длина токена = 5.26, SD = 3.83\n",
            "ANO_BB.cha: средняя длина токена = 5.08, SD = 3.68\n",
            "TYL_BB.cha: средняя длина токена = 5.14, SD = 3.85\n",
            "VDL_BB.cha: средняя длина токена = 5.09, SD = 3.89\n",
            "PPZ_BB.cha: средняя длина токена = 4.73, SD = 3.51\n",
            "AVG_BB.cha: средняя длина токена = 4.73, SD = 3.69\n",
            "MRP_BB.cha: средняя длина токена = 4.72, SD = 3.54\n",
            "AVM_BB.cha: средняя длина токена = 5.16, SD = 3.91\n",
            "GAA_BB.cha: средняя длина токена = 5.20, SD = 3.86\n",
            "SVK_BB.cha: средняя длина токена = 4.77, SD = 3.54\n",
            "EAB_BB.cha: средняя длина токена = 4.72, SD = 3.59\n",
            "TAR_BB.cha: средняя длина токена = 4.86, SD = 3.62\n",
            "PIF_BB.cha: средняя длина токена = 5.38, SD = 3.70\n",
            "OAR_BB.cha: средняя длина токена = 4.93, SD = 3.53\n",
            "EVP_BB.cha: средняя длина токена = 5.16, SD = 3.98\n",
            "AGM_BB.cha: средняя длина токена = 5.14, SD = 3.74\n",
            "VLN_BB.cha: средняя длина токена = 4.95, SD = 3.81\n",
            "IGM_BB.cha: средняя длина токена = 4.89, SD = 3.72\n",
            "EGI_BB.cha: средняя длина токена = 4.93, SD = 3.71\n",
            "TAL_BB.cha: средняя длина токена = 5.08, SD = 3.74\n",
            "MAV_BB.cha: средняя длина токена = 4.60, SD = 3.61\n",
            "ZVH_BB.cha: средняя длина токена = 4.89, SD = 3.49\n",
            "TNK_BB.cha: средняя длина токена = 5.10, SD = 3.88\n",
            "AIL_BB.cha: средняя длина токена = 4.97, SD = 3.77\n",
            "VES_BB.cha: средняя длина токена = 4.66, SD = 3.70\n",
            "HAL_BB.cha: средняя длина токена = 5.04, SD = 3.55\n",
            "EAK_BB.cha: средняя длина токена = 5.36, SD = 3.96\n",
            "NRP_BB.cha: средняя длина токена = 4.77, SD = 3.63\n",
            "\n",
            "Средняя длина токена по всем файлам: 4.93\n",
            "Стандартное отклонение длины токенов по всем файлам: 3.73\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Папка с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "print(f\"Найдено {len(file_list)} файлов в папке {folder_path}.\")\n",
        "\n",
        "# Списки для хранения результатов\n",
        "file_avg_lengths = []      # Средняя длина токена по каждому файлу\n",
        "file_std_lengths = []      # Стандартное отклонение по каждому файлу\n",
        "all_token_lengths = []     # Все токены всех файлов\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    token_lengths = []\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(r\"\\*{0}:\\s*\".format(speaker_prefix), \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = cleaned_line.split()\n",
        "            token_lengths.extend([len(token) for token in tokens])\n",
        "\n",
        "    if token_lengths:\n",
        "        avg_length = np.mean(token_lengths)\n",
        "        std_length = np.std(token_lengths)\n",
        "        file_avg_lengths.append(avg_length)\n",
        "        file_std_lengths.append(std_length)\n",
        "        all_token_lengths.extend(token_lengths)\n",
        "        print(f\"{filename}: средняя длина токена = {avg_length:.2f}, SD = {std_length:.2f}\")\n",
        "    else:\n",
        "        print(f\"{filename}: нет данных для анализа\")\n",
        "\n",
        "# Общие метрики\n",
        "if all_token_lengths:\n",
        "    overall_avg = np.mean(all_token_lengths)\n",
        "    overall_sd = np.std(all_token_lengths)\n",
        "    print(f\"\\nСредняя длина токена по всем файлам: {overall_avg:.2f}\")\n",
        "    print(f\"Стандартное отклонение длины токенов по всем файлам: {overall_sd:.2f}\")\n",
        "else:\n",
        "    print(\"Нет токенов для анализа.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiUHDOvaoSza",
        "outputId": "4358ef3c-dffc-43eb-9d8f-6f2c567b0aaa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.12/dist-packages (2.0.4)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (75.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import re\n",
        "import pymorphy3\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "folder_path = \"files\"  # Укажи правильный путь к папке с .cha файлами\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "vowels = set('аеёиоуыэюя')\n",
        "all_vowel_counts = []\n",
        "file_results = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    vowel_counts = []\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = cleaned_line.split()\n",
        "\n",
        "            for token in tokens:\n",
        "                parsed = morph.parse(token)\n",
        "                if parsed:\n",
        "                    normal_form = parsed[0].normal_form\n",
        "                    vowels_in_token = sum(1 for ch in normal_form.lower() if ch in vowels)\n",
        "                    vowel_counts.append(vowels_in_token)\n",
        "\n",
        "    if vowel_counts:\n",
        "        avg_vowels = np.mean(vowel_counts)\n",
        "        std_vowels = np.std(vowel_counts)\n",
        "        file_results.append((filename, avg_vowels, std_vowels))\n",
        "        all_vowel_counts.extend(vowel_counts)\n",
        "    else:\n",
        "        file_results.append((filename, 0, 0))\n",
        "\n",
        "# Вывод по каждому файлу\n",
        "for fname, avg_vow, std_vow in file_results:\n",
        "    print(f\"{fname}: среднее число гласных = {avg_vow:.2f}, SD = {std_vow:.2f}\")\n",
        "\n",
        "# Общие показатели по всем файлам\n",
        "if all_vowel_counts:\n",
        "    print(f\"\\nОбщее среднее число гласных в токене: {np.mean(all_vowel_counts):.2f}\")\n",
        "    print(f\"Общее стандартное отклонение числа гласных: {np.std(all_vowel_counts):.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DTooFyRnW0c",
        "outputId": "2c029fb4-a49f-41da-fa9f-91844a97af61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVM_BB.cha: среднее число гласных = 1.30, SD = 1.21\n",
            "LDL_BB.cha: среднее число гласных = 1.33, SD = 1.16\n",
            "ENSh_BB.cha: среднее число гласных = 1.41, SD = 1.10\n",
            "NVT-M_BB.cha: среднее число гласных = 1.46, SD = 1.22\n",
            "MAL_BB.cha: среднее число гласных = 1.33, SD = 1.19\n",
            "ANO_BB.cha: среднее число гласных = 1.46, SD = 1.15\n",
            "TYL_BB.cha: среднее число гласных = 1.34, SD = 1.18\n",
            "VDL_BB.cha: среднее число гласных = 1.26, SD = 1.19\n",
            "PPZ_BB.cha: среднее число гласных = 1.36, SD = 1.09\n",
            "AVG_BB.cha: среднее число гласных = 1.34, SD = 1.14\n",
            "MRP_BB.cha: среднее число гласных = 1.35, SD = 1.03\n",
            "AVM_BB.cha: среднее число гласных = 1.38, SD = 1.19\n",
            "GAA_BB.cha: среднее число гласных = 1.36, SD = 1.18\n",
            "SVK_BB.cha: среднее число гласных = 1.38, SD = 1.09\n",
            "EAB_BB.cha: среднее число гласных = 1.34, SD = 1.07\n",
            "TAR_BB.cha: среднее число гласных = 1.38, SD = 1.09\n",
            "PIF_BB.cha: среднее число гласных = 1.51, SD = 1.24\n",
            "OAR_BB.cha: среднее число гласных = 1.45, SD = 1.16\n",
            "EVP_BB.cha: среднее число гласных = 1.34, SD = 1.17\n",
            "AGM_BB.cha: среднее число гласных = 1.38, SD = 1.26\n",
            "VLN_BB.cha: среднее число гласных = 1.34, SD = 1.15\n",
            "IGM_BB.cha: среднее число гласных = 1.39, SD = 1.15\n",
            "EGI_BB.cha: среднее число гласных = 1.38, SD = 1.17\n",
            "TAL_BB.cha: среднее число гласных = 1.32, SD = 1.11\n",
            "MAV_BB.cha: среднее число гласных = 1.31, SD = 1.15\n",
            "ZVH_BB.cha: среднее число гласных = 1.45, SD = 1.14\n",
            "TNK_BB.cha: среднее число гласных = 1.43, SD = 1.19\n",
            "AIL_BB.cha: среднее число гласных = 1.34, SD = 1.17\n",
            "VES_BB.cha: среднее число гласных = 1.30, SD = 1.13\n",
            "HAL_BB.cha: среднее число гласных = 1.44, SD = 1.14\n",
            "EAK_BB.cha: среднее число гласных = 1.38, SD = 1.17\n",
            "NRP_BB.cha: среднее число гласных = 1.38, SD = 1.16\n",
            "\n",
            "Общее среднее число гласных в токене: 1.36\n",
            "Общее стандартное отклонение числа гласных: 1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "print(f\"Найдено {len(file_list)} файлов в папке {folder_path}.\")\n",
        "\n",
        "# Загрузка словаря морфем\n",
        "morpheme_dictionary = {}\n",
        "with open(\"cleaned_tihonov (5) (1).txt\", 'r', encoding='utf-8') as dict_file:\n",
        "    for line in dict_file:\n",
        "        line = line.strip()\n",
        "        if ' ' in line:\n",
        "            word, morphemes = line.split(' ', 1)\n",
        "            morpheme_count = morphemes.count('/') + 1\n",
        "            morpheme_dictionary[word.lower()] = morpheme_count\n",
        "\n",
        "# Основной цикл по файлам\n",
        "file_results = []\n",
        "all_morpheme_counts = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    morpheme_counts_in_file = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                token_lower = token.lower()\n",
        "                if token_lower in morpheme_dictionary:\n",
        "                    morpheme_count = morpheme_dictionary[token_lower]\n",
        "                    morpheme_counts_in_file.append(morpheme_count)\n",
        "\n",
        "    if morpheme_counts_in_file:\n",
        "        avg_morpheme_count = np.mean(morpheme_counts_in_file)\n",
        "        std_morpheme_count = np.std(morpheme_counts_in_file)\n",
        "        file_results.append((filename, avg_morpheme_count, std_morpheme_count))\n",
        "        all_morpheme_counts.extend(morpheme_counts_in_file)\n",
        "    else:\n",
        "        file_results.append((filename, 0, 0))\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, avg_morph, std_morph in file_results:\n",
        "    print(f\"{fname}: среднее количество морфем = {avg_morph:.2f}, SD = {std_morph:.2f}\")\n",
        "\n",
        "# Общие значения по всем файлам\n",
        "if all_morpheme_counts:\n",
        "    overall_avg = np.mean(all_morpheme_counts)\n",
        "    overall_sd = np.std(all_morpheme_counts)\n",
        "    print(f\"\\nСреднее количество морфем на токен по всем файлам: {overall_avg:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества морфем по всем файлам: {overall_sd:.2f}\")\n",
        "else:\n",
        "    print(\"Нет токенов, найденных в словаре морфем.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bkbYDG9hoLr",
        "outputId": "513ac7fc-7561-4e40-a884-c0eeb9ad28e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 32 файлов в папке files.\n",
            "NVM_BB.cha: среднее количество морфем = 1.57, SD = 0.86\n",
            "LDL_BB.cha: среднее количество морфем = 1.58, SD = 0.82\n",
            "ENSh_BB.cha: среднее количество морфем = 1.64, SD = 0.81\n",
            "NVT-M_BB.cha: среднее количество морфем = 1.66, SD = 0.83\n",
            "MAL_BB.cha: среднее количество морфем = 1.62, SD = 0.78\n",
            "ANO_BB.cha: среднее количество морфем = 1.57, SD = 0.81\n",
            "TYL_BB.cha: среднее количество морфем = 1.55, SD = 0.79\n",
            "VDL_BB.cha: среднее количество морфем = 1.63, SD = 0.84\n",
            "PPZ_BB.cha: среднее количество морфем = 1.51, SD = 0.74\n",
            "AVG_BB.cha: среднее количество морфем = 1.59, SD = 0.82\n",
            "MRP_BB.cha: среднее количество морфем = 1.57, SD = 0.78\n",
            "AVM_BB.cha: среднее количество морфем = 1.69, SD = 0.79\n",
            "GAA_BB.cha: среднее количество морфем = 1.67, SD = 0.85\n",
            "SVK_BB.cha: среднее количество морфем = 1.55, SD = 0.74\n",
            "EAB_BB.cha: среднее количество морфем = 1.52, SD = 0.75\n",
            "TAR_BB.cha: среднее количество морфем = 1.60, SD = 0.78\n",
            "PIF_BB.cha: среднее количество морфем = 1.67, SD = 0.80\n",
            "OAR_BB.cha: среднее количество морфем = 1.51, SD = 0.78\n",
            "EVP_BB.cha: среднее количество морфем = 1.65, SD = 0.87\n",
            "AGM_BB.cha: среднее количество морфем = 1.70, SD = 0.91\n",
            "VLN_BB.cha: среднее количество морфем = 1.60, SD = 0.86\n",
            "IGM_BB.cha: среднее количество морфем = 1.63, SD = 0.86\n",
            "EGI_BB.cha: среднее количество морфем = 1.59, SD = 0.85\n",
            "TAL_BB.cha: среднее количество морфем = 1.67, SD = 0.78\n",
            "MAV_BB.cha: среднее количество морфем = 1.61, SD = 0.89\n",
            "ZVH_BB.cha: среднее количество морфем = 1.72, SD = 0.86\n",
            "TNK_BB.cha: среднее количество морфем = 1.63, SD = 0.84\n",
            "AIL_BB.cha: среднее количество морфем = 1.67, SD = 0.90\n",
            "VES_BB.cha: среднее количество морфем = 1.51, SD = 0.77\n",
            "HAL_BB.cha: среднее количество морфем = 1.63, SD = 0.85\n",
            "EAK_BB.cha: среднее количество морфем = 1.67, SD = 0.81\n",
            "NRP_BB.cha: среднее количество морфем = 1.63, SD = 0.87\n",
            "\n",
            "Среднее количество морфем на токен по всем файлам: 1.61\n",
            "Стандартное отклонение количества морфем по всем файлам: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pymorphy3\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "# Результаты\n",
        "file_results = []\n",
        "unique_counts = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    unique_tokens = set()\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = re.findall(r'\\b\\w+\\b', cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token.strip():\n",
        "                    parsed = morph_analyzer.parse(token.lower())[0]\n",
        "                    normal_form = parsed.normal_form\n",
        "                    pos_tag = str(parsed.tag)\n",
        "                    token_info = f\"{normal_form} {pos_tag}\"\n",
        "                    unique_tokens.add(token_info)\n",
        "\n",
        "    unique_count = len(unique_tokens)\n",
        "    file_results.append((filename, unique_count))\n",
        "    unique_counts.append(unique_count)\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, uniq in file_results:\n",
        "    print(f\"{fname}: количество уникальных токенов (lemma+POS) = {uniq}\")\n",
        "\n",
        "# Подсчёт среднего и SD по всем файлам\n",
        "if unique_counts:\n",
        "    avg_unique = np.mean(unique_counts)\n",
        "    std_unique = np.std(unique_counts)\n",
        "    print(f\"\\nСреднее количество уникальных токенов по всем файлам: {avg_unique:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества уникальных токенов: {std_unique:.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdnbyR3jlMOt",
        "outputId": "0260ceec-8a42-419b-ecbe-acf7e677fb65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVM_BB.cha: количество уникальных токенов (lemma+POS) = 192\n",
            "LDL_BB.cha: количество уникальных токенов (lemma+POS) = 199\n",
            "ENSh_BB.cha: количество уникальных токенов (lemma+POS) = 153\n",
            "NVT-M_BB.cha: количество уникальных токенов (lemma+POS) = 284\n",
            "MAL_BB.cha: количество уникальных токенов (lemma+POS) = 230\n",
            "ANO_BB.cha: количество уникальных токенов (lemma+POS) = 165\n",
            "TYL_BB.cha: количество уникальных токенов (lemma+POS) = 214\n",
            "VDL_BB.cha: количество уникальных токенов (lemma+POS) = 259\n",
            "PPZ_BB.cha: количество уникальных токенов (lemma+POS) = 185\n",
            "AVG_BB.cha: количество уникальных токенов (lemma+POS) = 322\n",
            "MRP_BB.cha: количество уникальных токенов (lemma+POS) = 175\n",
            "AVM_BB.cha: количество уникальных токенов (lemma+POS) = 176\n",
            "GAA_BB.cha: количество уникальных токенов (lemma+POS) = 193\n",
            "SVK_BB.cha: количество уникальных токенов (lemma+POS) = 291\n",
            "EAB_BB.cha: количество уникальных токенов (lemma+POS) = 490\n",
            "TAR_BB.cha: количество уникальных токенов (lemma+POS) = 210\n",
            "PIF_BB.cha: количество уникальных токенов (lemma+POS) = 168\n",
            "OAR_BB.cha: количество уникальных токенов (lemma+POS) = 181\n",
            "EVP_BB.cha: количество уникальных токенов (lemma+POS) = 289\n",
            "AGM_BB.cha: количество уникальных токенов (lemma+POS) = 233\n",
            "VLN_BB.cha: количество уникальных токенов (lemma+POS) = 386\n",
            "IGM_BB.cha: количество уникальных токенов (lemma+POS) = 263\n",
            "EGI_BB.cha: количество уникальных токенов (lemma+POS) = 281\n",
            "TAL_BB.cha: количество уникальных токенов (lemma+POS) = 225\n",
            "MAV_BB.cha: количество уникальных токенов (lemma+POS) = 451\n",
            "ZVH_BB.cha: количество уникальных токенов (lemma+POS) = 278\n",
            "TNK_BB.cha: количество уникальных токенов (lemma+POS) = 294\n",
            "AIL_BB.cha: количество уникальных токенов (lemma+POS) = 456\n",
            "VES_BB.cha: количество уникальных токенов (lemma+POS) = 338\n",
            "HAL_BB.cha: количество уникальных токенов (lemma+POS) = 169\n",
            "EAK_BB.cha: количество уникальных токенов (lemma+POS) = 185\n",
            "NRP_BB.cha: количество уникальных токенов (lemma+POS) = 331\n",
            "\n",
            "Среднее количество уникальных токенов по всем файлам: 258.31\n",
            "Стандартное отклонение количества уникальных токенов: 88.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pymorphy3\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "# Результаты\n",
        "file_results = []\n",
        "all_unique_sets = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    unique_tokens = set()\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = re.findall(r'\\b\\w+\\b', cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token.strip():\n",
        "                    parsed = morph_analyzer.parse(token.lower())[0]\n",
        "                    normal_form = parsed.normal_form\n",
        "                    pos_tag = str(parsed.tag)\n",
        "                    token_info = normal_form\n",
        "                    unique_tokens.add(token_info)\n",
        "\n",
        "    unique_count = len(unique_tokens)\n",
        "    file_results.append((filename, unique_count))\n",
        "    all_unique_sets.append(unique_tokens)\n",
        "\n",
        "# Вывод результатов по файлам\n",
        "for fname, uniq in file_results:\n",
        "    print(f\"{fname}: количество уникальных лемм (lemma+POS) = {uniq}\")\n",
        "\n",
        "# Подсчёт среднего количества уникальных токенов по всем файлам\n",
        "if all_unique_sets:\n",
        "    avg_unique = sum(len(s) for s in all_unique_sets) / len(all_unique_sets)\n",
        "    std_unique = np.std(unique_counts)\n",
        "    print(f\"\\nСреднее количество уникальных лемм по всем файлам: {avg_unique:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества уникальных токенов: {std_unique:.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDTZaXremkD_",
        "outputId": "148fd5da-9567-4cba-9dc6-cd550fc0b7fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVM_BB.cha: количество уникальных лемм (lemma+POS) = 174\n",
            "LDL_BB.cha: количество уникальных лемм (lemma+POS) = 172\n",
            "ENSh_BB.cha: количество уникальных лемм (lemma+POS) = 135\n",
            "NVT-M_BB.cha: количество уникальных лемм (lemma+POS) = 234\n",
            "MAL_BB.cha: количество уникальных лемм (lemma+POS) = 193\n",
            "ANO_BB.cha: количество уникальных лемм (lemma+POS) = 134\n",
            "TYL_BB.cha: количество уникальных лемм (lemma+POS) = 180\n",
            "VDL_BB.cha: количество уникальных лемм (lemma+POS) = 216\n",
            "PPZ_BB.cha: количество уникальных лемм (lemma+POS) = 150\n",
            "AVG_BB.cha: количество уникальных лемм (lemma+POS) = 261\n",
            "MRP_BB.cha: количество уникальных лемм (lemma+POS) = 145\n",
            "AVM_BB.cha: количество уникальных лемм (lemma+POS) = 150\n",
            "GAA_BB.cha: количество уникальных лемм (lemma+POS) = 153\n",
            "SVK_BB.cha: количество уникальных лемм (lemma+POS) = 238\n",
            "EAB_BB.cha: количество уникальных лемм (lemma+POS) = 393\n",
            "TAR_BB.cha: количество уникальных лемм (lemma+POS) = 169\n",
            "PIF_BB.cha: количество уникальных лемм (lemma+POS) = 142\n",
            "OAR_BB.cha: количество уникальных лемм (lemma+POS) = 152\n",
            "EVP_BB.cha: количество уникальных лемм (lemma+POS) = 245\n",
            "AGM_BB.cha: количество уникальных лемм (lemma+POS) = 188\n",
            "VLN_BB.cha: количество уникальных лемм (lemma+POS) = 304\n",
            "IGM_BB.cha: количество уникальных лемм (lemma+POS) = 224\n",
            "EGI_BB.cha: количество уникальных лемм (lemma+POS) = 219\n",
            "TAL_BB.cha: количество уникальных лемм (lemma+POS) = 182\n",
            "MAV_BB.cha: количество уникальных лемм (lemma+POS) = 363\n",
            "ZVH_BB.cha: количество уникальных лемм (lemma+POS) = 227\n",
            "TNK_BB.cha: количество уникальных лемм (lemma+POS) = 239\n",
            "AIL_BB.cha: количество уникальных лемм (lemma+POS) = 360\n",
            "VES_BB.cha: количество уникальных лемм (lemma+POS) = 279\n",
            "HAL_BB.cha: количество уникальных лемм (lemma+POS) = 144\n",
            "EAK_BB.cha: количество уникальных лемм (lemma+POS) = 158\n",
            "NRP_BB.cha: количество уникальных лемм (lemma+POS) = 259\n",
            "\n",
            "Среднее количество уникальных лемм по всем файлам: 211.94\n",
            "Стандартное отклонение количества уникальных токенов: 88.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhJl11mkqIy1",
        "outputId": "d63beba9-204c-4b69-b806-d1b0c70d1002"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lexicalrichness in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from lexicalrichness) (1.16.2)\n",
            "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.12/dist-packages (from lexicalrichness) (0.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from lexicalrichness) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lexicalrichness) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.0.0->lexicalrichness) (2.0.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.9.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lexicalrichness) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->lexicalrichness) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->lexicalrichness) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lexicalrichness) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from lexicalrichness import LexicalRichness\n",
        "import numpy as np\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "file_results = []\n",
        "mtld_values = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            line_tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "            tokens.extend(line_tokens)\n",
        "\n",
        "    if len(tokens) >= 50:\n",
        "        text_for_mtld = \" \".join(tokens)\n",
        "        lex = LexicalRichness(text_for_mtld)\n",
        "        mtld_score = lex.mtld(threshold=0.72)\n",
        "        mtld_values.append(mtld_score)\n",
        "        file_results.append((filename, mtld_score))\n",
        "    else:\n",
        "        file_results.append((filename, None))  # недостаточно данных\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, mtld in file_results:\n",
        "    if mtld is not None:\n",
        "        print(f\"{fname}: MTLD = {mtld:.2f}\")\n",
        "    else:\n",
        "        print(f\"{fname}: слишком мало токенов для расчёта MTLD (<50)\")\n",
        "\n",
        "# Среднее значение MTLD и стандартное отклонение по всем подходящим файлам\n",
        "if mtld_values:\n",
        "    avg_mtld = np.mean(mtld_values)\n",
        "    std_mtld = np.std(mtld_values)\n",
        "    print(f\"\\nСреднее значение MTLD по всем файлам: {avg_mtld:.2f}\")\n",
        "    print(f\"Стандартное отклонение MTLD по всем файлам: {std_mtld:.2f}\")\n",
        "else:\n",
        "    print(\"Недостаточно данных для расчёта MTLD.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAEKfNpBqORn",
        "outputId": "9b72ef0d-91d2-4422-ad54-59c85beecddb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVM_BB.cha: MTLD = 61.30\n",
            "LDL_BB.cha: MTLD = 70.67\n",
            "ENSh_BB.cha: MTLD = 75.13\n",
            "NVT-M_BB.cha: MTLD = 80.94\n",
            "MAL_BB.cha: MTLD = 99.38\n",
            "ANO_BB.cha: MTLD = 94.57\n",
            "TYL_BB.cha: MTLD = 88.07\n",
            "VDL_BB.cha: MTLD = 64.91\n",
            "PPZ_BB.cha: MTLD = 109.70\n",
            "AVG_BB.cha: MTLD = 75.97\n",
            "MRP_BB.cha: MTLD = 70.59\n",
            "AVM_BB.cha: MTLD = 96.92\n",
            "GAA_BB.cha: MTLD = 87.85\n",
            "SVK_BB.cha: MTLD = 66.60\n",
            "EAB_BB.cha: MTLD = 94.11\n",
            "TAR_BB.cha: MTLD = 77.89\n",
            "PIF_BB.cha: MTLD = 81.85\n",
            "OAR_BB.cha: MTLD = 83.06\n",
            "EVP_BB.cha: MTLD = 94.22\n",
            "AGM_BB.cha: MTLD = 105.77\n",
            "VLN_BB.cha: MTLD = 114.00\n",
            "IGM_BB.cha: MTLD = 62.35\n",
            "EGI_BB.cha: MTLD = 84.56\n",
            "TAL_BB.cha: MTLD = 67.08\n",
            "MAV_BB.cha: MTLD = 77.85\n",
            "ZVH_BB.cha: MTLD = 99.45\n",
            "TNK_BB.cha: MTLD = 102.71\n",
            "AIL_BB.cha: MTLD = 84.40\n",
            "VES_BB.cha: MTLD = 99.09\n",
            "HAL_BB.cha: MTLD = 55.74\n",
            "EAK_BB.cha: MTLD = 63.58\n",
            "NRP_BB.cha: MTLD = 79.94\n",
            "\n",
            "Среднее значение MTLD по всем файлам: 83.45\n",
            "Стандартное отклонение MTLD по всем файлам: 15.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "from lexicalrichness import LexicalRichness\n",
        "import pymorphy3\n",
        "import numpy as np\n",
        "\n",
        "# Морфоанализатор\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Папка с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "file_results = []\n",
        "mtld_values = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    lemmas = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*PAR0:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*PAR0:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                parsed = morph.parse(token)[0]\n",
        "                lemmas.append(parsed.normal_form)\n",
        "\n",
        "    if len(lemmas) >= 50:\n",
        "        text_for_mtld = \" \".join(lemmas)\n",
        "        lex = LexicalRichness(text_for_mtld)\n",
        "        mtld_score = lex.mtld(threshold=0.72)\n",
        "        mtld_values.append(mtld_score)\n",
        "        file_results.append((filename, mtld_score))\n",
        "    else:\n",
        "        file_results.append((filename, None))\n",
        "\n",
        "# Вывод результатов\n",
        "for fname, mtld in file_results:\n",
        "    if mtld is not None:\n",
        "        print(f\"{fname}: MTLD по леммам = {mtld:.2f}\")\n",
        "    else:\n",
        "        print(f\"{fname}: недостаточно лемм для расчёта MTLD (<50)\")\n",
        "\n",
        "# Среднее значение и стандартное отклонение MTLD по всем подходящим файлам\n",
        "if mtld_values:\n",
        "    avg_mtld = np.mean(mtld_values)\n",
        "    std_mtld = np.std(mtld_values)\n",
        "    print(f\"\\nСреднее MTLD по леммам по всем файлам: {avg_mtld:.2f}\")\n",
        "    print(f\"Стандартное отклонение MTLD по леммам по всем файлам: {std_mtld:.2f}\")\n",
        "else:\n",
        "    print(\"Недостаточно данных для расчёта MTLD.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sib3HAtiqcQ7",
        "outputId": "91428364-af4e-48ff-87ce-77f40b709c3b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVM_BB.cha: MTLD по леммам = 42.43\n",
            "LDL_BB.cha: MTLD по леммам = 40.70\n",
            "ENSh_BB.cha: MTLD по леммам = 51.28\n",
            "NVT-M_BB.cha: MTLD по леммам = 47.39\n",
            "MAL_BB.cha: MTLD по леммам = 52.85\n",
            "ANO_BB.cha: MTLD по леммам = 46.57\n",
            "TYL_BB.cha: MTLD по леммам = 52.49\n",
            "VDL_BB.cha: MTLD по леммам = 53.63\n",
            "PPZ_BB.cha: MTLD по леммам = 41.54\n",
            "AVG_BB.cha: MTLD по леммам = 45.82\n",
            "MRP_BB.cha: MTLD по леммам = 43.49\n",
            "AVM_BB.cha: MTLD по леммам = 63.70\n",
            "GAA_BB.cha: MTLD по леммам = 42.51\n",
            "SVK_BB.cha: MTLD по леммам = 42.66\n",
            "EAB_BB.cha: MTLD по леммам = 61.72\n",
            "TAR_BB.cha: MTLD по леммам = 47.06\n",
            "PIF_BB.cha: MTLD по леммам = 35.93\n",
            "OAR_BB.cha: MTLD по леммам = 57.93\n",
            "EVP_BB.cha: MTLD по леммам = 50.65\n",
            "AGM_BB.cha: MTLD по леммам = 50.74\n",
            "VLN_BB.cha: MTLD по леммам = 57.71\n",
            "IGM_BB.cha: MTLD по леммам = 47.98\n",
            "EGI_BB.cha: MTLD по леммам = 47.41\n",
            "TAL_BB.cha: MTLD по леммам = 41.37\n",
            "MAV_BB.cha: MTLD по леммам = 54.36\n",
            "ZVH_BB.cha: MTLD по леммам = 51.55\n",
            "TNK_BB.cha: MTLD по леммам = 55.11\n",
            "AIL_BB.cha: MTLD по леммам = 60.87\n",
            "VES_BB.cha: MTLD по леммам = 63.83\n",
            "HAL_BB.cha: MTLD по леммам = 42.67\n",
            "EAK_BB.cha: MTLD по леммам = 42.85\n",
            "NRP_BB.cha: MTLD по леммам = 43.24\n",
            "\n",
            "Среднее MTLD по леммам по всем файлам: 49.38\n",
            "Стандартное отклонение MTLD по леммам по всем файлам: 7.22\n"
          ]
        }
      ]
    }
  ]
}
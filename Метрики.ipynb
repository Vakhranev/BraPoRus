{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK8Dz/lgMP8SO2+ubvGvU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/BraPoRus/blob/main/%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NQvk8aS04Yu",
        "outputId": "5e85c909-23ba-4377-8492-c9b771ec7255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 16 файлов в папке files.\n",
            "TNK_S5.cha: средняя длина токена = 4.82, SD = 4.16\n",
            "FFK_S1.cha: средняя длина токена = 4.59, SD = 3.82\n",
            "SAP_S1.cha: средняя длина токена = 4.64, SD = 4.12\n",
            "ZVH_S1.cha: средняя длина токена = 4.70, SD = 3.89\n",
            "AVM_S1.cha: средняя длина токена = 4.47, SD = 3.68\n",
            "SAK_S2.cha: средняя длина токена = 4.76, SD = 3.92\n",
            "IMK_S3.cha: средняя длина токена = 4.77, SD = 4.21\n",
            "TYL_S1.cha: средняя длина токена = 5.11, SD = 4.36\n",
            "VVG_S1.cha: средняя длина токена = 5.21, SD = 4.40\n",
            "ENL_S1.cha: средняя длина токена = 4.18, SD = 3.66\n",
            "NVM_S3.cha: средняя длина токена = 4.68, SD = 4.03\n",
            "EKS_S4.cha: средняя длина токена = 4.93, SD = 4.14\n",
            "GAA_S1.cha: средняя длина токена = 4.32, SD = 3.74\n",
            "AVG_S2.cha: средняя длина токена = 5.06, SD = 4.26\n",
            "LNI_S5.cha: средняя длина токена = 4.28, SD = 3.47\n",
            "EAB_S9.cha: средняя длина токена = 4.42, SD = 3.65\n",
            "\n",
            "Средняя длина токена по всем файлам: 4.65\n",
            "Стандартное отклонение длины токенов по всем файлам: 3.96\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Папка с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "print(f\"Найдено {len(file_list)} файлов в папке {folder_path}.\")\n",
        "\n",
        "# Списки для хранения результатов\n",
        "file_avg_lengths = []      # Средняя длина токена по каждому файлу\n",
        "file_std_lengths = []      # Стандартное отклонение по каждому файлу\n",
        "all_token_lengths = []     # Все токены всех файлов\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    token_lengths = []\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(r\"\\*{0}:\\s*\".format(speaker_prefix), \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = cleaned_line.split()\n",
        "            token_lengths.extend([len(token) for token in tokens])\n",
        "\n",
        "    if token_lengths:\n",
        "        avg_length = np.mean(token_lengths)\n",
        "        std_length = np.std(token_lengths)\n",
        "        file_avg_lengths.append(avg_length)\n",
        "        file_std_lengths.append(std_length)\n",
        "        all_token_lengths.extend(token_lengths)\n",
        "        print(f\"{filename}: средняя длина токена = {avg_length:.2f}, SD = {std_length:.2f}\")\n",
        "    else:\n",
        "        print(f\"{filename}: нет данных для анализа\")\n",
        "\n",
        "# Общие метрики\n",
        "if all_token_lengths:\n",
        "    overall_avg = np.mean(all_token_lengths)\n",
        "    overall_sd = np.std(all_token_lengths)\n",
        "    print(f\"\\nСредняя длина токена по всем файлам: {overall_avg:.2f}\")\n",
        "    print(f\"Стандартное отклонение длины токенов по всем файлам: {overall_sd:.2f}\")\n",
        "else:\n",
        "    print(\"Нет токенов для анализа.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiUHDOvaoSza",
        "outputId": "c0df6282-e093-4c93-d0c6-33ed8d722ded"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import re\n",
        "import pymorphy3\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "folder_path = \"files\"  # Укажи правильный путь к папке с .cha файлами\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "vowels = set('аеёиоуыэюя')\n",
        "all_vowel_counts = []\n",
        "file_results = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    vowel_counts = []\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = cleaned_line.split()\n",
        "\n",
        "            for token in tokens:\n",
        "                parsed = morph.parse(token)\n",
        "                if parsed:\n",
        "                    normal_form = parsed[0].normal_form\n",
        "                    vowels_in_token = sum(1 for ch in normal_form.lower() if ch in vowels)\n",
        "                    vowel_counts.append(vowels_in_token)\n",
        "\n",
        "    if vowel_counts:\n",
        "        avg_vowels = np.mean(vowel_counts)\n",
        "        std_vowels = np.std(vowel_counts)\n",
        "        file_results.append((filename, avg_vowels, std_vowels))\n",
        "        all_vowel_counts.extend(vowel_counts)\n",
        "    else:\n",
        "        file_results.append((filename, 0, 0))\n",
        "\n",
        "# Вывод по каждому файлу\n",
        "for fname, avg_vow, std_vow in file_results:\n",
        "    print(f\"{fname}: среднее число гласных = {avg_vow:.2f}, SD = {std_vow:.2f}\")\n",
        "\n",
        "# Общие показатели по всем файлам\n",
        "if all_vowel_counts:\n",
        "    print(f\"\\nОбщее среднее число гласных в токене: {np.mean(all_vowel_counts):.2f}\")\n",
        "    print(f\"Общее стандартное отклонение числа гласных: {np.std(all_vowel_counts):.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DTooFyRnW0c",
        "outputId": "a91a125a-5cc0-46c9-ee17-f1ad8e233474"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TNK_S5.cha: среднее число гласных = 1.28, SD = 1.22\n",
            "FFK_S1.cha: среднее число гласных = 1.30, SD = 1.13\n",
            "SAP_S1.cha: среднее число гласных = 1.26, SD = 1.15\n",
            "ZVH_S1.cha: среднее число гласных = 1.31, SD = 1.16\n",
            "AVM_S1.cha: среднее число гласных = 1.35, SD = 1.15\n",
            "SAK_S2.cha: среднее число гласных = 1.34, SD = 1.10\n",
            "IMK_S3.cha: среднее число гласных = 1.20, SD = 1.12\n",
            "TYL_S1.cha: среднее число гласных = 1.24, SD = 1.19\n",
            "VVG_S1.cha: среднее число гласных = 1.25, SD = 1.25\n",
            "ENL_S1.cha: среднее число гласных = 1.21, SD = 1.07\n",
            "NVM_S3.cha: среднее число гласных = 1.25, SD = 1.19\n",
            "EKS_S4.cha: среднее число гласных = 1.29, SD = 1.13\n",
            "GAA_S1.cha: среднее число гласных = 1.24, SD = 1.11\n",
            "AVG_S2.cha: среднее число гласных = 1.36, SD = 1.29\n",
            "LNI_S5.cha: среднее число гласных = 1.26, SD = 1.09\n",
            "EAB_S9.cha: среднее число гласных = 1.34, SD = 1.08\n",
            "\n",
            "Общее среднее число гласных в токене: 1.28\n",
            "Общее стандартное отклонение числа гласных: 1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "print(f\"Найдено {len(file_list)} файлов в папке {folder_path}.\")\n",
        "\n",
        "# Загрузка словаря морфем\n",
        "morpheme_dictionary = {}\n",
        "with open(\"cleaned_tihonov (5) (1).txt\", 'r', encoding='utf-8') as dict_file:\n",
        "    for line in dict_file:\n",
        "        line = line.strip()\n",
        "        if ' ' in line:\n",
        "            word, morphemes = line.split(' ', 1)\n",
        "            morpheme_count = morphemes.count('/') + 1\n",
        "            morpheme_dictionary[word.lower()] = morpheme_count\n",
        "\n",
        "# Основной цикл по файлам\n",
        "file_results = []\n",
        "all_morpheme_counts = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    morpheme_counts_in_file = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                token_lower = token.lower()\n",
        "                if token_lower in morpheme_dictionary:\n",
        "                    morpheme_count = morpheme_dictionary[token_lower]\n",
        "                    morpheme_counts_in_file.append(morpheme_count)\n",
        "\n",
        "    if morpheme_counts_in_file:\n",
        "        avg_morpheme_count = np.mean(morpheme_counts_in_file)\n",
        "        std_morpheme_count = np.std(morpheme_counts_in_file)\n",
        "        file_results.append((filename, avg_morpheme_count, std_morpheme_count))\n",
        "        all_morpheme_counts.extend(morpheme_counts_in_file)\n",
        "    else:\n",
        "        file_results.append((filename, 0, 0))\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, avg_morph, std_morph in file_results:\n",
        "    print(f\"{fname}: среднее количество морфем = {avg_morph:.2f}, SD = {std_morph:.2f}\")\n",
        "\n",
        "# Общие значения по всем файлам\n",
        "if all_morpheme_counts:\n",
        "    overall_avg = np.mean(all_morpheme_counts)\n",
        "    overall_sd = np.std(all_morpheme_counts)\n",
        "    print(f\"\\nСреднее количество морфем на токен по всем файлам: {overall_avg:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества морфем по всем файлам: {overall_sd:.2f}\")\n",
        "else:\n",
        "    print(\"Нет токенов, найденных в словаре морфем.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bkbYDG9hoLr",
        "outputId": "7257dcb7-19b3-463c-f827-6c98e5de2ff4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 16 файлов в папке files.\n",
            "TNK_S5.cha: среднее количество морфем = 1.44, SD = 0.71\n",
            "FFK_S1.cha: среднее количество морфем = 1.48, SD = 0.65\n",
            "SAP_S1.cha: среднее количество морфем = 1.46, SD = 0.80\n",
            "ZVH_S1.cha: среднее количество морфем = 1.48, SD = 0.76\n",
            "AVM_S1.cha: среднее количество морфем = 1.43, SD = 0.73\n",
            "SAK_S2.cha: среднее количество морфем = 1.49, SD = 0.73\n",
            "IMK_S3.cha: среднее количество морфем = 1.50, SD = 0.79\n",
            "TYL_S1.cha: среднее количество морфем = 1.39, SD = 0.67\n",
            "VVG_S1.cha: среднее количество морфем = 1.39, SD = 0.66\n",
            "ENL_S1.cha: среднее количество морфем = 1.42, SD = 0.71\n",
            "NVM_S3.cha: среднее количество морфем = 1.48, SD = 0.75\n",
            "EKS_S4.cha: среднее количество морфем = 1.47, SD = 0.71\n",
            "GAA_S1.cha: среднее количество морфем = 1.38, SD = 0.72\n",
            "AVG_S2.cha: среднее количество морфем = 1.46, SD = 0.71\n",
            "LNI_S5.cha: среднее количество морфем = 1.61, SD = 0.83\n",
            "EAB_S9.cha: среднее количество морфем = 1.48, SD = 0.65\n",
            "\n",
            "Среднее количество морфем на токен по всем файлам: 1.47\n",
            "Стандартное отклонение количества морфем по всем файлам: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pymorphy3\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "# Результаты\n",
        "file_results = []\n",
        "unique_counts = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    unique_tokens = set()\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = re.findall(r'\\b\\w+\\b', cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token.strip():\n",
        "                    parsed = morph_analyzer.parse(token.lower())[0]\n",
        "                    normal_form = parsed.normal_form\n",
        "                    pos_tag = str(parsed.tag)\n",
        "                    token_info = f\"{normal_form} {pos_tag}\"\n",
        "                    unique_tokens.add(token_info)\n",
        "\n",
        "    unique_count = len(unique_tokens)\n",
        "    file_results.append((filename, unique_count))\n",
        "    unique_counts.append(unique_count)\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, uniq in file_results:\n",
        "    print(f\"{fname}: количество уникальных токенов (lemma+POS) = {uniq}\")\n",
        "\n",
        "# Подсчёт среднего и SD по всем файлам\n",
        "if unique_counts:\n",
        "    avg_unique = np.mean(unique_counts)\n",
        "    std_unique = np.std(unique_counts)\n",
        "    print(f\"\\nСреднее количество уникальных токенов по всем файлам: {avg_unique:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества уникальных токенов: {std_unique:.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdnbyR3jlMOt",
        "outputId": "4c4cc771-e0c5-469d-8748-41b4f7851d99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TNK_S5.cha: количество уникальных токенов (lemma+POS) = 501\n",
            "FFK_S1.cha: количество уникальных токенов (lemma+POS) = 589\n",
            "SAP_S1.cha: количество уникальных токенов (lemma+POS) = 538\n",
            "ZVH_S1.cha: количество уникальных токенов (lemma+POS) = 1086\n",
            "AVM_S1.cha: количество уникальных токенов (lemma+POS) = 644\n",
            "SAK_S2.cha: количество уникальных токенов (lemma+POS) = 635\n",
            "IMK_S3.cha: количество уникальных токенов (lemma+POS) = 665\n",
            "TYL_S1.cha: количество уникальных токенов (lemma+POS) = 705\n",
            "VVG_S1.cha: количество уникальных токенов (lemma+POS) = 517\n",
            "ENL_S1.cha: количество уникальных токенов (lemma+POS) = 627\n",
            "NVM_S3.cha: количество уникальных токенов (lemma+POS) = 544\n",
            "EKS_S4.cha: количество уникальных токенов (lemma+POS) = 544\n",
            "GAA_S1.cha: количество уникальных токенов (lemma+POS) = 653\n",
            "AVG_S2.cha: количество уникальных токенов (lemma+POS) = 683\n",
            "LNI_S5.cha: количество уникальных токенов (lemma+POS) = 552\n",
            "EAB_S9.cha: количество уникальных токенов (lemma+POS) = 535\n",
            "\n",
            "Среднее количество уникальных токенов по всем файлам: 626.12\n",
            "Стандартное отклонение количества уникальных токенов: 134.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pymorphy3\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "# Результаты\n",
        "file_results = []\n",
        "all_unique_sets = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    unique_tokens = set()\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "\n",
        "            tokens = re.findall(r'\\b\\w+\\b', cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token.strip():\n",
        "                    parsed = morph_analyzer.parse(token.lower())[0]\n",
        "                    normal_form = parsed.normal_form\n",
        "                    pos_tag = str(parsed.tag)\n",
        "                    token_info = normal_form\n",
        "                    unique_tokens.add(token_info)\n",
        "\n",
        "    unique_count = len(unique_tokens)\n",
        "    file_results.append((filename, unique_count))\n",
        "    all_unique_sets.append(unique_tokens)\n",
        "\n",
        "# Вывод результатов по файлам\n",
        "for fname, uniq in file_results:\n",
        "    print(f\"{fname}: количество уникальных лемм (lemma+POS) = {uniq}\")\n",
        "\n",
        "# Подсчёт среднего количества уникальных токенов по всем файлам\n",
        "if all_unique_sets:\n",
        "    avg_unique = sum(len(s) for s in all_unique_sets) / len(all_unique_sets)\n",
        "    std_unique = np.std(unique_counts)\n",
        "    print(f\"\\nСреднее количество уникальных лемм по всем файлам: {avg_unique:.2f}\")\n",
        "    print(f\"Стандартное отклонение количества уникальных токенов: {std_unique:.2f}\")\n",
        "else:\n",
        "    print(\"Нет данных для анализа.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDTZaXremkD_",
        "outputId": "3b873d0c-92de-48a7-8b53-1939743f71f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TNK_S5.cha: количество уникальных лемм (lemma+POS) = 411\n",
            "FFK_S1.cha: количество уникальных лемм (lemma+POS) = 466\n",
            "SAP_S1.cha: количество уникальных лемм (lemma+POS) = 426\n",
            "ZVH_S1.cha: количество уникальных лемм (lemma+POS) = 850\n",
            "AVM_S1.cha: количество уникальных лемм (lemma+POS) = 503\n",
            "SAK_S2.cha: количество уникальных лемм (lemma+POS) = 512\n",
            "IMK_S3.cha: количество уникальных лемм (lemma+POS) = 523\n",
            "TYL_S1.cha: количество уникальных лемм (lemma+POS) = 579\n",
            "VVG_S1.cha: количество уникальных лемм (lemma+POS) = 430\n",
            "ENL_S1.cha: количество уникальных лемм (lemma+POS) = 507\n",
            "NVM_S3.cha: количество уникальных лемм (lemma+POS) = 454\n",
            "EKS_S4.cha: количество уникальных лемм (lemma+POS) = 435\n",
            "GAA_S1.cha: количество уникальных лемм (lemma+POS) = 509\n",
            "AVG_S2.cha: количество уникальных лемм (lemma+POS) = 572\n",
            "LNI_S5.cha: количество уникальных лемм (lemma+POS) = 433\n",
            "EAB_S9.cha: количество уникальных лемм (lemma+POS) = 431\n",
            "\n",
            "Среднее количество уникальных лемм по всем файлам: 502.56\n",
            "Стандартное отклонение количества уникальных токенов: 134.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhJl11mkqIy1",
        "outputId": "20391a05-fce2-4b2a-bff9-96e3b417bf44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexicalrichness\n",
            "  Downloading lexicalrichness-0.5.1.tar.gz (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m92.2/97.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from lexicalrichness) (1.15.3)\n",
            "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.11/dist-packages (from lexicalrichness) (0.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from lexicalrichness) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lexicalrichness) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.0.0->lexicalrichness) (2.0.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.9.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lexicalrichness) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->lexicalrichness) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->lexicalrichness) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob>=0.15.3->lexicalrichness) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lexicalrichness) (1.17.0)\n",
            "Building wheels for collected packages: lexicalrichness\n",
            "  Building wheel for lexicalrichness (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lexicalrichness: filename=lexicalrichness-0.5.1-py3-none-any.whl size=15418 sha256=6325700baed0741cf41991fd21568133e9a868c6e425b6506d1b2af55f169ac5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/68/f4/1a32ae0aae29ce426b4e3c106a4e8e9c19fd13bcaff45c6a8e\n",
            "Successfully built lexicalrichness\n",
            "Installing collected packages: lexicalrichness\n",
            "Successfully installed lexicalrichness-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from lexicalrichness import LexicalRichness\n",
        "import numpy as np\n",
        "\n",
        "# Путь к папке с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "file_results = []\n",
        "mtld_values = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            line_tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "            tokens.extend(line_tokens)\n",
        "\n",
        "    if len(tokens) >= 50:\n",
        "        text_for_mtld = \" \".join(tokens)\n",
        "        lex = LexicalRichness(text_for_mtld)\n",
        "        mtld_score = lex.mtld(threshold=0.72)\n",
        "        mtld_values.append(mtld_score)\n",
        "        file_results.append((filename, mtld_score))\n",
        "    else:\n",
        "        file_results.append((filename, None))  # недостаточно данных\n",
        "\n",
        "# Вывод результатов по каждому файлу\n",
        "for fname, mtld in file_results:\n",
        "    if mtld is not None:\n",
        "        print(f\"{fname}: MTLD = {mtld:.2f}\")\n",
        "    else:\n",
        "        print(f\"{fname}: слишком мало токенов для расчёта MTLD (<50)\")\n",
        "\n",
        "# Среднее значение MTLD и стандартное отклонение по всем подходящим файлам\n",
        "if mtld_values:\n",
        "    avg_mtld = np.mean(mtld_values)\n",
        "    std_mtld = np.std(mtld_values)\n",
        "    print(f\"\\nСреднее значение MTLD по всем файлам: {avg_mtld:.2f}\")\n",
        "    print(f\"Стандартное отклонение MTLD по всем файлам: {std_mtld:.2f}\")\n",
        "else:\n",
        "    print(\"Недостаточно данных для расчёта MTLD.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAEKfNpBqORn",
        "outputId": "8891d8fe-160e-40cf-f2e7-babc5ab6df0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TNK_S5.cha: MTLD = 48.21\n",
            "FFK_S1.cha: MTLD = 80.89\n",
            "SAP_S1.cha: MTLD = 47.76\n",
            "ZVH_S1.cha: MTLD = 74.44\n",
            "AVM_S1.cha: MTLD = 67.83\n",
            "SAK_S2.cha: MTLD = 64.33\n",
            "IMK_S3.cha: MTLD = 54.05\n",
            "TYL_S1.cha: MTLD = 69.46\n",
            "VVG_S1.cha: MTLD = 62.24\n",
            "ENL_S1.cha: MTLD = 55.62\n",
            "NVM_S3.cha: MTLD = 71.11\n",
            "EKS_S4.cha: MTLD = 69.40\n",
            "GAA_S1.cha: MTLD = 70.16\n",
            "AVG_S2.cha: MTLD = 73.48\n",
            "LNI_S5.cha: MTLD = 45.08\n",
            "EAB_S9.cha: MTLD = 54.82\n",
            "\n",
            "Среднее значение MTLD по всем файлам: 63.06\n",
            "Стандартное отклонение MTLD по всем файлам: 10.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "from lexicalrichness import LexicalRichness\n",
        "import pymorphy3\n",
        "import numpy as np\n",
        "\n",
        "# Морфоанализатор\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Папка с .cha файлами\n",
        "folder_path = \"files\"\n",
        "file_list = glob.glob(os.path.join(folder_path, \"*.cha\"))\n",
        "\n",
        "file_results = []\n",
        "mtld_values = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    speaker_prefix = filename[:3]\n",
        "\n",
        "    lemmas = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(f\"*{speaker_prefix}:\"):\n",
        "            cleaned_line = re.sub(rf\"\\*{speaker_prefix}:\\s*\", \"\", line)\n",
        "            cleaned_line = re.sub(r\"•\\d+_\\d+•\", \"\", cleaned_line)\n",
        "            cleaned_line = cleaned_line.strip()\n",
        "            tokens = re.findall(r\"\\b\\w+\\b\", cleaned_line)\n",
        "\n",
        "            for token in tokens:\n",
        "                parsed = morph.parse(token)[0]\n",
        "                lemmas.append(parsed.normal_form)\n",
        "\n",
        "    if len(lemmas) >= 50:\n",
        "        text_for_mtld = \" \".join(lemmas)\n",
        "        lex = LexicalRichness(text_for_mtld)\n",
        "        mtld_score = lex.mtld(threshold=0.72)\n",
        "        mtld_values.append(mtld_score)\n",
        "        file_results.append((filename, mtld_score))\n",
        "    else:\n",
        "        file_results.append((filename, None))\n",
        "\n",
        "# Вывод результатов\n",
        "for fname, mtld in file_results:\n",
        "    if mtld is not None:\n",
        "        print(f\"{fname}: MTLD по леммам = {mtld:.2f}\")\n",
        "    else:\n",
        "        print(f\"{fname}: недостаточно лемм для расчёта MTLD (<50)\")\n",
        "\n",
        "# Среднее значение и стандартное отклонение MTLD по всем подходящим файлам\n",
        "if mtld_values:\n",
        "    avg_mtld = np.mean(mtld_values)\n",
        "    std_mtld = np.std(mtld_values)\n",
        "    print(f\"\\nСреднее MTLD по леммам по всем файлам: {avg_mtld:.2f}\")\n",
        "    print(f\"Стандартное отклонение MTLD по леммам по всем файлам: {std_mtld:.2f}\")\n",
        "else:\n",
        "    print(\"Недостаточно данных для расчёта MTLD.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sib3HAtiqcQ7",
        "outputId": "1b32a524-bad7-4d09-cd72-1c8b32016c23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TNK_S5.cha: MTLD по леммам = 30.75\n",
            "FFK_S1.cha: MTLD по леммам = 47.05\n",
            "SAP_S1.cha: MTLD по леммам = 31.19\n",
            "ZVH_S1.cha: MTLD по леммам = 51.59\n",
            "AVM_S1.cha: MTLD по леммам = 49.19\n",
            "SAK_S2.cha: MTLD по леммам = 38.39\n",
            "IMK_S3.cha: MTLD по леммам = 30.54\n",
            "TYL_S1.cha: MTLD по леммам = 41.34\n",
            "VVG_S1.cha: MTLD по леммам = 35.29\n",
            "ENL_S1.cha: MTLD по леммам = 34.73\n",
            "NVM_S3.cha: MTLD по леммам = 52.37\n",
            "EKS_S4.cha: MTLD по леммам = 42.06\n",
            "GAA_S1.cha: MTLD по леммам = 40.98\n",
            "AVG_S2.cha: MTLD по леммам = 45.16\n",
            "LNI_S5.cha: MTLD по леммам = 35.23\n",
            "EAB_S9.cha: MTLD по леммам = 33.48\n",
            "\n",
            "Среднее MTLD по леммам по всем файлам: 39.96\n",
            "Стандартное отклонение MTLD по леммам по всем файлам: 7.18\n"
          ]
        }
      ]
    }
  ]
}